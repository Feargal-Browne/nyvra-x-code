import os
import warnings
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import shutil
import json
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from torch.cuda.amp import GradScaler, autocast
from safetensors.torch import load_file
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from google.colab import userdata
from huggingface_hub import login, hf_hub_download

warnings.filterwarnings("ignore")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

CONFIG = {
    "BASE_MODEL_NAMES": ["Feargal/de_berta_model", "Feargal/distil-roberta"],
    "DATASET_PATH": "news_df.csv",
    "TEXT_COLUMN": "text",
    "LABEL_COLUMN": "label",
    "OOF_PREDS_PATH": "oof_predictions.npy",
    "OOF_METADATA_PATH": "oof_metadata.json",
    "OOF_FOLDS": 5,
    "QUANTUM_CHECKPOINT_DIR": "quantum_checkpoints",
    "QUANTUM_BEST_MODEL_PATH": "quantum_checkpoints/quantum_oof_BEST.pth",
    "QUANTUM_EPOCHS": 25,
}


class BayesLinear(nn.Module):
    def __init__(self, in_features, out_features, prior_sigma=1.0, kl_scale=1.0):
        super().__init__()
        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))
        self.weight_rho = nn.Parameter(torch.full((out_features, in_features), -5.0))
        self.bias_mu = nn.Parameter(torch.zeros(out_features))
        self.bias_rho = nn.Parameter(torch.full((out_features,), -5.0))
        self.prior_sigma = prior_sigma
        self.kl_scale = kl_scale

    def forward(self, x, sample=False):
        if not sample:
            weight, bias = self.weight_mu, self.bias_mu
        else:
            weight_sigma = torch.log1p(torch.exp(self.weight_rho))
            bias_sigma = torch.log1p(torch.exp(self.bias_rho))
            weight = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)
            bias = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)
        return F.linear(x, weight, bias)


class RobertaForSequenceClassificationCustom(nn.Module):
    def __init__(self, model_name, num_labels=2):
        super().__init__()
        self.base = AutoModel.from_pretrained(model_name)
        self.classifier = BayesLinear(self.base.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]
        logits = self.classifier(pooled_output, sample=False)
        return type("obj", (object,), {"logits": logits})


class QuantumInspiredNeuralEnsemble(nn.Module):
    def __init__(self, n_models=2, hidden_dim=64, n_quantum_layers=3, dropout_rate=0.2):
        super().__init__()
        self.n_models = n_models
        self.n_quantum_layers = n_quantum_layers
        self.rotation_params = nn.Parameter(torch.randn(n_quantum_layers, n_models) * 0.1)
        self.entanglement_params = nn.Parameter(
            torch.randn(n_quantum_layers, max(1, n_models - 1)) * 0.1
        )
        self.ensemble_net = nn.Sequential(
            nn.Linear(n_models, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, 2),
        )
        self.uncertainty_net = nn.Sequential(
            nn.Linear(n_models, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid(),
        )
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
            if hasattr(m, "bias") and m.bias is not None:
                nn.init.zeros_(m.bias)

    def quantum_inspired_transform(self, preds):
        t_preds = preds.clone()
        for layer in range(self.n_quantum_layers):
            rotated_preds = t_preds.clone()
            for i in range(self.n_models):
                theta = self.rotation_params[layer, i]
                rotated_preds[:, i] = torch.clamp(
                    torch.cos(theta) * t_preds[:, i]
                    + torch.sin(theta) * (1 - t_preds[:, i]),
                    0,
                    1,
                )
            t_preds = rotated_preds

            if self.n_models > 1:
                entangled_preds = t_preds.clone()
                for i in range(self.n_models - 1):
                    phi = self.entanglement_params[layer, i]
                    phase = torch.cos(phi * t_preds[:, i] * t_preds[:, i + 1])
                    entangled_preds[:, i] = t_preds[:, i] * phase
                    entangled_preds[:, i + 1] = t_preds[:, i + 1] * phase
                t_preds = entangled_preds
        return t_preds

    def forward(self, model_predictions):
        qt = self.quantum_inspired_transform(model_predictions)
        return self.ensemble_net(qt), self.uncertainty_net(qt)


def login_to_hf(token):
    try:
        login(token=token)
        print("Successfully logged into Hugging Face Hub.")
    except Exception as e:
        print(f"Failed to log in: {e}")
        raise


def load_base_models(model_names, hf_token):
    models, tokenizers = {}, {}
    for name in model_names:
        print(f"Loading model and tokenizer for: {name}...")
        tokenizer = AutoTokenizer.from_pretrained(name, token=hf_token)
        if "distil-roberta" in name.lower():
            print("Custom DistilRoBERTa model detected; using unified weight loading.")
            model = RobertaForSequenceClassificationCustom(name).to(DEVICE)
            try:
                weights_path = hf_hub_download(repo_id=name, filename="model.safetensors", token=hf_token)
                state_dict = load_file(weights_path, device=str(DEVICE))
                model.load_state_dict(state_dict)
                print("Successfully loaded fine-tuned weights into the custom model.")
            except Exception as e:
                print(
                    f"Warning: Could not perform unified load for '{name}'. "
                    f"Model may use pre-trained base weights with an untrained head. Error: {e}"
                )
        else:
            print("Standard model detected; using AutoModelForSequenceClassification.")
            model = AutoModelForSequenceClassification.from_pretrained(name, token=hf_token).to(DEVICE)
        model.eval()
        models[name] = model
        tokenizers[name] = tokenizer
        print(f"Loaded {name} successfully.")
    return models, tokenizers


def generate_predictions_batch(texts, model, tokenizer, batch_size=128):
    model.eval()
    all_probs = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Generating Preds", leave=False):
        batch_texts = texts[i : i + batch_size]
        inputs = tokenizer(
            batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512
        ).to(DEVICE)
        with torch.no_grad(), autocast():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()
            all_probs.extend(probs)
    return np.array(all_probs)


def save_checkpoint(epoch, model, optimizer, best_val_acc, folder):
    os.makedirs(folder, exist_ok=True)
    metadata = {"epoch": epoch + 1, "best_val_acc": best_val_acc}
    with open(os.path.join(folder, "metadata.json"), "w") as f:
        json.dump(metadata, f)
    torch.save(model.state_dict(), os.path.join(folder, "model_last.pth"))
    torch.save(optimizer.state_dict(), os.path.join(folder, "optimizer_last.pt"))


def load_checkpoint(model, optimizer, folder):
    metadata_path = os.path.join(folder, "metadata.json")
    if not os.path.exists(metadata_path):
        return 0, 0.0
    with open(metadata_path, "r") as f:
        metadata = json.load(f)
    model.load_state_dict(torch.load(os.path.join(folder, "model_last.pth")))
    optimizer.load_state_dict(torch.load(os.path.join(folder, "optimizer_last.pt")))
    print(f"Resumed from checkpoint. Starting from epoch {metadata['epoch']}.")
    return metadata["epoch"], metadata["best_val_acc"]


def run_stage_1_oof_generation(df, text_col, label_col, models, tokenizers):
    print("STAGE 1: Generating OOF predictions")
    if os.path.exists(CONFIG["OOF_PREDS_PATH"]):
        choice = input("Found existing OOF predictions. Reset and run again? (yes/no) [default: no]: ") or "no"
        if choice.strip().lower() == "yes":
            os.remove(CONFIG["OOF_PREDS_PATH"])
            if os.path.exists(CONFIG["OOF_METADATA_PATH"]):
                os.remove(CONFIG["OOF_METADATA_PATH"])
            print("Reset OOF predictions.")

    if os.path.exists(CONFIG["OOF_PREDS_PATH"]) and os.path.exists(CONFIG["OOF_METADATA_PATH"]):
        oof_preds = np.load(CONFIG["OOF_PREDS_PATH"])
        with open(CONFIG["OOF_METADATA_PATH"], "r") as f:
            start_fold = json.load(f)["last_completed_fold"] + 1
        print(f"Resuming OOF generation from Fold {start_fold + 1}.")
    else:
        oof_preds = np.zeros((len(df), len(models)))
        start_fold = 0
        print("Starting new OOF generation.")

    if start_fold >= CONFIG["OOF_FOLDS"]:
        print("OOF prediction generation already complete.")
        return oof_preds, df[label_col].values

    skf = StratifiedKFold(n_splits=CONFIG["OOF_FOLDS"], shuffle=True, random_state=42)
    texts, labels = df[text_col].tolist(), df[label_col].values
    folds = list(skf.split(np.zeros(len(texts)), labels))

    for fold in range(start_fold, CONFIG["OOF_FOLDS"]):
        print(f"Processing Fold {fold + 1}/{CONFIG['OOF_FOLDS']}")
        _, val_idx = folds[fold]
        val_texts = [texts[i] for i in val_idx]
        for model_idx, (model_name, model) in enumerate(models.items()):
            print(f"Predicting with {model_name}...")
            val_preds = generate_predictions_batch(val_texts, model, tokenizers[model_name])
            oof_preds[val_idx, model_idx] = val_preds
        np.save(CONFIG["OOF_PREDS_PATH"], oof_preds)
        with open(CONFIG["OOF_METADATA_PATH"], "w") as f:
            json.dump({"last_completed_fold": fold}, f)
        print(f"Saved progress for Fold {fold + 1}.")

    print("OOF prediction generation complete.")
    return oof_preds, labels


def run_stage_2_quantum_training(X_meta, y_meta):
    print("Stage 2: Training quantum ensemble")
    total_epochs = 0
    choice = input("Would you like to train the quantum model? (yes/no) [default: yes]: ") or "yes"
    if choice.strip().lower() == "yes":
        while True:
            try:
                num_epochs_str = input("For how many epochs? (e.g., 15, 25, 35): ")
                total_epochs = int(num_epochs_str)
                if total_epochs > 0:
                    break
                else:
                    print("Please enter a positive number.")
            except ValueError:
                print("Invalid input. Please enter a number.")
    else:
        print("Skipping quantum model training.")
        return

    skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    train_idx, val_idx = next(skf_meta.split(X_meta, y_meta))
    X_train_meta, X_val_meta = X_meta[train_idx], X_meta[val_idx]
    y_train_meta, y_val_meta = y_meta[train_idx], y_meta[val_idx]

    train_dataset = TensorDataset(
        torch.tensor(X_train_meta, dtype=torch.float32), torch.tensor(y_train_meta, dtype=torch.long)
    )
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)

    X_val_tensor = torch.tensor(X_val_meta, dtype=torch.float32).to(DEVICE)
    y_val_tensor = torch.tensor(y_val_meta, dtype=torch.long).to(DEVICE)

    model = QuantumInspiredNeuralEnsemble().to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    scaler = GradScaler()
    criterion = nn.CrossEntropyLoss()

    start_epoch, best_val_acc = load_checkpoint(model, optimizer, CONFIG["QUANTUM_CHECKPOINT_DIR"])
    end_epoch = start_epoch + total_epochs
    print(f"Training from epoch {start_epoch + 1} to {end_epoch}...")

    for epoch in range(start_epoch, end_epoch):
        model.train()
        progress_bar = tqdm(train_loader, desc=f"Quantum Epoch {epoch + 1}/{end_epoch}", leave=False)
        for batch_X, batch_y in progress_bar:
            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
            optimizer.zero_grad()
            with autocast():
                logits, uncertainty = model(batch_X)
                loss = criterion(logits, batch_y) + 0.01 * torch.mean(uncertainty)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        model.eval()
        with torch.no_grad():
            val_logits, _ = model(X_val_tensor)
            val_preds = torch.argmax(val_logits, dim=1)
            val_accuracy = accuracy_score(y_val_tensor.cpu(), val_preds.cpu())
            print(f"Quantum Epoch {epoch + 1}/{end_epoch} | Val Accuracy: {val_accuracy:.4f}")
            save_checkpoint(epoch, model, optimizer, best_val_acc, CONFIG["QUANTUM_CHECKPOINT_DIR"])
            if val_accuracy > best_val_acc:
                best_val_acc = val_accuracy
                torch.save(model.state_dict(), CONFIG["QUANTUM_BEST_MODEL_PATH"])
                print(f"New best quantum model saved with accuracy: {best_val_acc:.4f}")

    print("Quantum ensemble training complete.")


def main():
    try:
        HF_TOKEN = userdata.get("HF_TOKEN")
        login_to_hf(HF_TOKEN)
    except Exception:
        print("Could not load HF token.")
        return

    df = pd.read_csv(CONFIG["DATASET_PATH"])
    df[CONFIG["TEXT_COLUMN"]] = df[CONFIG["TEXT_COLUMN"]].fillna("").astype(str)

    le = LabelEncoder()
    df[CONFIG["LABEL_COLUMN"]] = le.fit_transform(df[CONFIG["LABEL_COLUMN"]])

    models, tokenizers = load_base_models(CONFIG["BASE_MODEL_NAMES"], HF_TOKEN)

    X_meta, y_meta = run_stage_1_oof_generation(
        df, CONFIG["TEXT_COLUMN"], CONFIG["LABEL_COLUMN"], models, tokenizers
    )

    run_stage_2_quantum_training(X_meta, y_meta)

    print("Final evaluation")

    if not os.path.exists(CONFIG["QUANTUM_BEST_MODEL_PATH"]):
        print("No best model found. Please train the quantum model first.")
        return

    final_model = QuantumInspiredNeuralEnsemble().to(DEVICE)
    final_model.load_state_dict(torch.load(CONFIG["QUANTUM_BEST_MODEL_PATH"]))
    final_model.eval()

    skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    _, val_idx = next(skf_meta.split(X_meta, y_meta))
    X_val_meta, y_val_meta = X_meta[val_idx], y_meta[val_idx]
    X_val_tensor = torch.tensor(X_val_meta, dtype=torch.float32).to(DEVICE)

    with torch.no_grad():
        final_logits, _ = final_model(X_val_tensor)
        final_preds = torch.argmax(final_logits, dim=1).cpu().numpy()

    print("Classification report (validation set):")
    target_names = [str(c) for c in le.classes_]
    print(classification_report(y_val_meta, final_preds, target_names=target_names, digits=7))
    print("Pipeline finished successfully.")


if __name__ == "__main__":
    main()

