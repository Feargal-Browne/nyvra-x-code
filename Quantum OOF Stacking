# Fine Tuning Quantum OOF Ensemble
# Core Imports 
import os
import warnings
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import shutil
import json
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from torch.cuda.amp import GradScaler, autocast
from safetensors.torch import load_file
# Scikit-learn 
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
# Hugging Face Login & Hub Downloader 
from google.colab import userdata
from huggingface_hub import login, hf_hub_download

# Model Configuration

warnings.filterwarnings('ignore')
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ Using device: {DEVICE}")
CONFIG = {
"BASE_MODEL_NAMES": ['Feargal/de_berta_model', 'Feargal/distil-roberta'],
"DATASET_PATH": 'news_df.csv',
"TEXT_COLUMN": 'text',
"LABEL_COLUMN": 'label',
"OOF_PREDS_PATH": "oof_predictions.npy",
"OOF_METADATA_PATH": "oof_metadata.json",
"OOF_FOLDS": 5,
"QUANTUM_CHECKPOINT_DIR": "quantum_checkpoints",
"QUANTUM_BEST_MODEL_PATH": "quantum_checkpoints/quantum_oof_BEST.pth",
"QUANTUM_EPOCHS": 25,
}

# Model definitions

# Bayesian Linear Layer 
class BayesLinear(nn.Module):
def __init__(self, in_features, out_features, prior_sigma=1.0, kl_scale=1.0):
super().__init__()
self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))
self.weight_rho = nn.Parameter(torch.full((out_features, in_features), -5.0))
self.bias_mu = nn.Parameter(torch.zeros(out_features))
self.bias_rho = nn.Parameter(torch.full((out_features,), -5.0))
self.prior_sigma, self.kl_scale = prior_sigma, kl_scale
def forward(self, x, sample=False): # Set sample=False for inference
if not sample:
weight, bias = self.weight_mu, self.bias_mu
else:
weight_sigma = torch.log1p(torch.exp(self.weight_rho))
bias_sigma = torch.log1p(torch.exp(self.bias_rho))
weight = self.weight_mu + weight_sigma * torch.randn_like(weight_sigma)
bias = self.bias_mu + bias_sigma * torch.randn_like(bias_sigma)
return F.linear(x, weight, bias)
===
#  DistilRoBERTa model with custom Bayesian Linear Layer Loading
class RobertaForSequenceClassificationCustom(nn.Module):
def __init__(self, model_name, num_labels=2):
super().__init__()
self.base = AutoModel.from_pretrained(model_name)
self.classifier = BayesLinear(self.base.config.hidden_size, num_labels)
def forward(self, input_ids, attention_mask):
outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)
pooled_output = outputs.last_hidden_state[:, 0]
logits = self.classifier(pooled_output, sample=False)
return type('obj', (object,), {'logits': logits})

class QuantumInspiredNeuralEnsemble(nn.Module):
def __init__(self, n_models=2, hidden_dim=64, n_quantum_layers=3, dropout_rate=0.2):
super().__init__()
self.n_models, self.n_quantum_layers = n_models, n_quantum_layers
self.rotation_params = nn.Parameter(torch.randn(n_quantum_layers, n_models) * 0.1)
self.entanglement_params = nn.Parameter(torch.randn(n_quantum_layers, max(1, n_models - 1)) * 0.1)
self.ensemble_net = nn.Sequential(
nn.Linear(n_models, hidden_dim), nn.GELU(), nn.Dropout(dropout_rate),
nn.Linear(hidden_dim, hidden_dim // 2), nn.GELU(), nn.Dropout(dropout_rate),
nn.Linear(hidden_dim // 2, 2))
self.uncertainty_net = nn.Sequential(
nn.Linear(n_models, hidden_dim // 2), nn.ReLU(),
nn.Linear(hidden_dim // 2, 1), nn.Sigmoid())
self._initialize_weights()
def _initialize_weights(self):
for m in self.modules():
if isinstance(m, nn.Linear):
nn.init.xavier_uniform_(m.weight)
if m.bias is not None: nn.init.zeros_(m.bias)
def quantum_inspired_transform(self, preds):
t_preds = preds.clone()
for layer in range(self.n_quantum_layers):
rotated_preds = t_preds.clone()
for i in range(self.n_models):
theta = self.rotation_params[layer, i]
rotated_preds[:, i] = torch.clamp(torch.cos(theta) * t_preds[:, i] + torch.sin(theta) * (1 - t_preds[:, i]), 0, 1)
t_preds = rotated_preds
# --- Step 2: Entanglement ---
if self.n_models > 1:
entangled_preds = t_preds.clone()
for i in range(self.n_models - 1):
phi = self.entanglement_params[layer, i]
phase = torch.cos(phi * t_preds[:, i] * t_preds[:, i + 1])
entangled_preds[:, i] = t_preds[:, i] * phase
entangled_preds[:, i + 1] = t_preds[:, i + 1] * phase
t_preds = entangled_preds
return t_preds
def forward(self, model_predictions):
qt = self.quantum_inspired_transform(model_predictions)
return self.ensemble_net(qt), self.uncertainty_net(qt)
# Hugging Face Login
def login_to_hf(token):
try:
login(token=token)
print("‚úÖ Successfully logged into Hugging Face Hub.")
except Exception as e:
print(f"‚ùå Failed to log in: {e}")
raise
def load_base_models(model_names, hf_token):
models, tokenizers = {}, {}
for name in model_names:
print(f"üîÑ Loading model and tokenizer for: {name}...")
tokenizer = AutoTokenizer.from_pretrained(name, token=hf_token)
if "distil-roberta" in name.lower():
print(f"Found custom DistilRoBERTa model. Using unified weight loading...")
model = RobertaForSequenceClassificationCustom(name).to(DEVICE)
try:
weights_path = hf_hub_download(
repo_id=name,
filename="model.safetensors",
token=hf_token
)
state_dict = load_file(weights_path, device=str(DEVICE))
model.load_state_dict(state_dict)
print("‚úÖ Successfully loaded all fine-tuned weights into the custom model.")
except Exception as e:
print(f"‚ö†Ô∏è WARNING: Could not perform unified load for '{name}'. "
f"The model may use pre-trained base weights with an untrained head. Error: {e}")
else:
print(f"Found standard model. Using AutoModelForSequenceClassification...")
model = AutoModelForSequenceClassification.from_pretrained(name, token=hf_token).to(DEVICE)
model.eval()
models[name] = model
tokenizers[name] = tokenizer
print(f"‚úÖ Loaded {name} successfully.")
return models, tokenizers
def generate_predictions_batch(texts, model, tokenizer, batch_size=128):
model.eval()
all_probs = []
for i in tqdm(range(0, len(texts), batch_size), desc="Generating Preds", leave=False):
batch_texts = texts[i:i + batch_size]
inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(DEVICE)
with torch.no_grad(), autocast():
outputs = model(**inputs)
logits = outputs.logits
probs = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()
all_probs.extend(probs)
return np.array(all_probs)
def save_checkpoint(epoch, model, optimizer, best_val_acc, folder):
os.makedirs(folder, exist_ok=True)
metadata = {'epoch': epoch + 1, 'best_val_acc': best_val_acc}
with open(os.path.join(folder, "metadata.json"), 'w') as f:
json.dump(metadata, f)
torch.save(model.state_dict(), os.path.join(folder, "model_last.pth"))
torch.save(optimizer.state_dict(), os.path.join(folder, "optimizer_last.pt"))
def load_checkpoint(model, optimizer, folder):
metadata_path = os.path.join(folder, "metadata.json")
if not os.path.exists(metadata_path):
return 0, 0.0
with open(metadata_path, 'r') as f:
metadata = json.load(f)
model.load_state_dict(torch.load(os.path.join(folder, "model_last.pth")))
optimizer.load_state_dict(torch.load(os.path.join(folder, "optimizer_last.pt")))
print(f"‚úÖ Resumed from checkpoint. Starting from epoch {metadata['epoch']}.")
return metadata['epoch'], metadata['best_val_acc']
def run_stage_1_oof_generation(df, text_col, label_col, models, tokenizers):
print("\n" + "="*80 + "\n=== STAGE 1: GENERATING OOF PREDICTIONS ===\n" + "="*80)
if os.path.exists(CONFIG['OOF_PREDS_PATH']):
choice = input("‚ùì Found existing OOF predictions. Reset and run again? (yes/no) [default: no]: ") or "no"
if choice.strip().lower() == "yes":
os.remove(CONFIG['OOF_PREDS_PATH'])
if os.path.exists(CONFIG['OOF_METADATA_PATH']):
os.remove(CONFIG['OOF_METADATA_PATH'])
print("üî• Reset OOF predictions.")
if os.path.exists(CONFIG['OOF_PREDS_PATH']) and os.path.exists(CONFIG['OOF_METADATA_PATH']):
oof_preds = np.load(CONFIG['OOF_PREDS_PATH'])
with open(CONFIG['OOF_METADATA_PATH'], 'r') as f:
start_fold = json.load(f)['last_completed_fold'] + 1
print(f"‚úÖ Resuming OOF generation from Fold {start_fold + 1}.")
else:
oof_preds = np.zeros((len(df), len(models)))
start_fold = 0
print("Starting new OOF generation.")
if start_fold >= CONFIG['OOF_FOLDS']:
print("‚úÖ OOF Prediction Generation is already complete.")
return oof_preds, df[label_col].values
skf = StratifiedKFold(n_splits=CONFIG['OOF_FOLDS'], shuffle=True, random_state=42)
texts, labels = df[text_col].tolist(), df[label_col].values
folds = list(skf.split(np.zeros(len(texts)), labels))
for fold in range(start_fold, CONFIG['OOF_FOLDS']):
print(f"\n--- Processing Fold {fold+1}/{CONFIG['OOF_FOLDS']} ---")
_, val_idx = folds[fold]
val_texts = [texts[i] for i in val_idx]
for model_idx, (model_name, model) in enumerate(models.items()):
print(f"Predicting with {model_name}...")
val_preds = generate_predictions_batch(val_texts, model, tokenizers[model_name])
oof_preds[val_idx, model_idx] = val_preds
np.save(CONFIG['OOF_PREDS_PATH'], oof_preds)
with open(CONFIG['OOF_METADATA_PATH'], 'w') as f:
json.dump({'last_completed_fold': fold}, f)
print(f"‚úÖ Saved progress for Fold {fold+1}.")
print("\n‚úÖ OOF Prediction Generation Complete.")
return oof_preds, labels
def run_stage_2_quantum_training(X_meta, y_meta):
print("\n" + "="*80 + "\n=== Stage 2: Training Quantum Ensemble ===\n" + "="*80)
total_epochs = 0
choice = input("‚ùì Would you like to train the quantum model? (yes/no) [default: yes]: ") or "yes"
if choice.strip().lower() == "yes":
while True:
try:
num_epochs_str = input("‚û°Ô∏è For how many epochs? (e.g., 15, 25, 35): ")
total_epochs = int(num_epochs_str)
if total_epochs > 0:
break
else:
print("Please enter a positive number.")
except ValueError:
print("Invalid input. Please enter a number.")
else:
print("Skipping quantum model training.")
return
skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
train_idx, val_idx = next(skf_meta.split(X_meta, y_meta))
X_train_meta, X_val_meta, y_train_meta, y_val_meta = X_meta[train_idx], X_meta[val_idx], y_meta[train_idx], y_meta[val_idx]
train_dataset = TensorDataset(torch.tensor(X_train_meta, dtype=torch.float32), torch.tensor(y_train_meta, dtype=torch.long))
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)
X_val_tensor, y_val_tensor = torch.tensor(X_val_meta, dtype=torch.float32).to(DEVICE), torch.tensor(y_val_meta, dtype=torch.long).to(DEVICE)
model = QuantumInspiredNeuralEnsemble().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=1e-4)
scaler = GradScaler()
criterion = nn.CrossEntropyLoss()
start_epoch, best_val_acc = load_checkpoint(model, optimizer, CONFIG['QUANTUM_CHECKPOINT_DIR'])
end_epoch = start_epoch + total_epochs
print(f"Training from epoch {start_epoch + 1} to {end_epoch}...")
for epoch in range(start_epoch, end_epoch):
model.train()
progress_bar = tqdm(train_loader, desc=f"Quantum Epoch {epoch+1}/{end_epoch}", leave=False)
for batch_X, batch_y in progress_bar:
batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
optimizer.zero_grad()
with autocast():
logits, uncertainty = model(batch_X)
loss = criterion(logits, batch_y) + 0.01 * torch.mean(uncertainty)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
model.eval()
with torch.no_grad():
val_logits, _ = model(X_val_tensor)
val_preds = torch.argmax(val_logits, dim=1)
val_accuracy = accuracy_score(y_val_tensor.cpu(), val_preds.cpu())
print(f"Quantum Epoch {epoch+1}/{end_epoch} | Val Accuracy: {val_accuracy:.4f}")
save_checkpoint(epoch, model, optimizer, best_val_acc, CONFIG['QUANTUM_CHECKPOINT_DIR'])
if val_accuracy > best_val_acc:
best_val_acc = val_accuracy
torch.save(model.state_dict(), CONFIG['QUANTUM_BEST_MODEL_PATH'])
print(f"üéâ New best quantum model saved with accuracy: {best_val_acc:.4f}")
print("‚úÖ Quantum Ensemble Training Complete.")
def main():
print("\n" + "-"*60 + "\nüöÄ Starting Quantum OOF Ensemble Pipeline üöÄ\n" + "-"*60)
try:
HF_TOKEN = userdata.get('HF_TOKEN')
login_to_hf(HF_TOKEN)
except Exception as e:
print("‚ùå Could not load HF token.")
return
df = pd.read_csv(CONFIG['DATASET_PATH'])
df[CONFIG['TEXT_COLUMN']] = df[CONFIG['TEXT_COLUMN']].fillna("").astype(str)
le = LabelEncoder()
df[CONFIG['LABEL_COLUMN']] = le.fit_transform(df[CONFIG['LABEL_COLUMN']])
models, tokenizers = load_base_models(CONFIG['BASE_MODEL_NAMES'], HF_TOKEN)
X_meta, y_meta = run_stage_1_oof_generation(df, CONFIG['TEXT_COLUMN'], CONFIG['LABEL_COLUMN'], models, tokenizers)
run_stage_2_quantum_training(X_meta, y_meta)
print("\n" + "-"*50 + "\nüìä Final Evaluation\n" + "-"*50)
if not os.path.exists(CONFIG['QUANTUM_BEST_MODEL_PATH']):
print("No best model found. Please train the quantum model first.")
return
final_model = QuantumInspiredNeuralEnsemble().to(DEVICE)
final_model.load_state_dict(torch.load(CONFIG['QUANTUM_BEST_MODEL_PATH']))
final_model.eval()
skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
_, val_idx = next(skf_meta.split(X_meta, y_meta))
X_val_meta, y_val_meta = X_meta[val_idx], y_meta[val_idx]
X_val_tensor = torch.tensor(X_val_meta, dtype=torch.float32).to(DEVICE)
with torch.no_grad():
final_logits, _ = final_model(X_val_tensor)
final_preds = torch.argmax(final_logits, dim=1).cpu().numpy()
print("\nClassification Report (on validation set):\n")
target_names = [str(c) for c in le.classes_]
print(classification_report(y_val_meta, final_preds, target_names=target_names, digits=7))
print("üéâ Pipeline Finished Successfully! üéâ")
if __name__ == '__main__':
main()
