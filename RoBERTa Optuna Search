# Imports and Setup
import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import optuna
from datasets import load_dataset, DatasetDict, Dataset
from transformers import (
    AutoTokenizer,
    AutoConfig,
    RobertaForSequenceClassification,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    default_data_collator,
    set_seed,
    EarlyStoppingCallback,
)
from sklearn.metrics import f1_score, accuracy_score
from peft import PeftModel
from huggingface_hub import login as hf_login
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger("datasets").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)

# Model & Data
MODEL_NAME = "roberta-base"
LORA_ADAPTER_REPO = "Feargal/roberta-lora-disinformation-v6"
TRAIN_TSV = "all_train.tsv"
VALID_TSV = "all_validate.tsv"
OUTPUT_DIR = "./optuna-for-roberta-stable"
CACHE_DIR = "./cache"
DB_PATH = os.path.join(OUTPUT_DIR, "optuna_study.db") 
NUM_LABELS = 6

# Optuna Search
NUM_TRIALS = 75
OPTUNA_TIMEOUT = 50 * 60
TRAIN_SUBSET_FRACTION = 0.15
VALIDATION_SUBSET_SEED = 42

# Training (default paramaters for free tier Colab T4 GPU)
MAX_TRAINING_STEPS = 500
EVAL_SAVE_STEPS = 500
EARLY_STOPPING_PATIENCE = 3
EVAL_BATCH_SIZE = 324
DATALOADER_NUM_WORKERS = os.cpu_count() // 2 if os.name == "posix" else 0

# Hugging Face Access Token
HF_TOKEN = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")
if HF_TOKEN:
    hf_login(token=HF_TOKEN)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE.upper()}")
print(f"Using {DATALOADER_NUM_WORKERS} dataloader workers.")

# Data Preperation (with Caching)
def load_and_preprocess_data(cache_path: str) -> DatasetDict:
    if os.path.exists(cache_path):
        print(f"‚úÖ Loading cleaned data from cache: {cache_path}")
        return DatasetDict.load_from_disk(cache_path)

    print("‚è≥ Loading and cleaning from source TSV files...")
    ds = load_dataset(
        "csv",
        data_files={"train": TRAIN_TSV, "validation": VALID_TSV},
        delimiter="\t",
    )

    def clean_and_select_columns(batch):
        batch["text"] = [
            str(t) if t is not None and not isinstance(t, float) else "" for t in batch["title"]
        ]
        batch["label"] = [int(l) for l in batch["6_way_label"]]
        return batch

    ds = ds.map(clean_and_select_columns, batched=True, num_proc=os.cpu_count() or 1)
    columns_to_keep = ["text", "label"]
    columns_to_remove = [col for col in ds["train"].column_names if col not in columns_to_keep]
    ds = ds.remove_columns(columns_to_remove)

    print(f"üíæ Saving cleaned data to cache: {cache_path}")
    ds.save_to_disk(cache_path)
    return ds


def tokenize_and_cache_dataset(ds: DatasetDict, tokenizer, max_len: int, cache_dir: str) -> DatasetDict:
    cache_path = os.path.join(cache_dir, f"tokenized_{MODEL_NAME.replace('/', '_')}_maxlen_{max_len}")
    if os.path.exists(cache_path):
        print(f"‚úÖ Loading tokenized data from cache: {cache_path}")
        return DatasetDict.load_from_disk(cache_path)

    print(f"‚è≥ Tokenizing data with max_length={max_len} and caching...")

    def tokenize(batch):
        return tokenizer(batch["text"], truncation=True, max_length=max_len, padding="max_length")

    tokenized_ds = ds.map(tokenize, batched=True, num_proc=os.cpu_count() or 1, remove_columns=["text"])
    print(f"üíæ Saving tokenized data to cache: {cache_path}")
    tokenized_ds.save_to_disk(cache_path)
    return tokenized_ds


# Custom Trainer and Metrics
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss


def compute_metrics(pred):
    preds = np.argmax(pred.predictions, axis=1)
    labels = pred.label_ids
    macro_f1 = f1_score(labels, preds, average="macro", zero_division=0)
    acc = accuracy_score(labels, preds)
    return {"macro_f1": macro_f1, "accuracy": acc}


# Optuna Set-up
class ReportToOptunaCallback(TrainerCallback):
    def __init__(self, trial: optuna.trial.Trial):
        self.trial = trial

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and "eval_macro_f1" in metrics:
            metric_to_report = metrics["eval_macro_f1"]
            self.trial.report(metric_to_report, state.global_step)
            if self.trial.should_prune():
                raise optuna.exceptions.TrialPruned()


def objective(trial: optuna.trial.Trial, clean_ds: DatasetDict) -> float:
    set_seed(42)

    # Hyperparameter search space (this will be used for subsequent trials)
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 2e-4, log=True)
    weight_decay = trial.suggest_float("weight_decay", 1e-8, 1e-2, log=True)
    adam_beta2 = trial.suggest_float("adam_beta2", 0.98, 0.999)
    dropout = trial.suggest_float("dropout", 0.05, 0.3)
    warmup_ratio = trial.suggest_float("warmup_ratio", 0.0, 0.25)
    scheduler = trial.suggest_categorical("scheduler", ["linear", "cosine", "cosine_with_restarts"])
    batch_size = trial.suggest_categorical("batch_size", [64, 128])
    grad_accum_factor = trial.suggest_categorical("grad_accum_factor", [1, 2, 4])
    max_seq_len = trial.suggest_categorical("max_seq_len", [128, 256])

    # Tokenization (uses caching)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenized_ds = tokenize_and_cache_dataset(
        ds=clean_ds, tokenizer=tokenizer, max_len=max_seq_len, cache_dir=CACHE_DIR
    )

    # Prepare Datasets
    train_full = tokenized_ds["train"].shuffle(seed=42)
    keep_n = max(1, int(len(train_full) * TRAIN_SUBSET_FRACTION))
    tokenized_train_subset = train_full.select(range(keep_n))
    tokenized_val = tokenized_ds["validation"]

    # Model Setup
    config = AutoConfig.from_pretrained(
        MODEL_NAME,
        num_labels=NUM_LABELS,
        hidden_dropout_prob=dropout,
        attention_probs_dropout_prob=dropout,
    )
    model = RobertaForSequenceClassification.from_pretrained(
        MODEL_NAME, config=config, attn_implementation="sdpa"  
    )
    model = PeftModel.from_pretrained(model, LORA_ADAPTER_REPO, is_trainable=True)

    # For a significant speedup on modern GPUs with PyTorch 2.0+, uncomment the following line. However, due to the instability of this function it has been excluded.
    # model = torch.compile(model)

    callbacks = [
        ReportToOptunaCallback(trial),
        EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE),
    ]

    training_args = TrainingArguments(
        output_dir=os.path.join(OUTPUT_DIR, f"trial_{trial.number}"),
        eval_strategy="steps",
        save_strategy="steps",
        eval_steps=EVAL_SAVE_STEPS,
        save_steps=EVAL_SAVE_STEPS,
        max_steps=MAX_TRAINING_STEPS,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        gradient_accumulation_steps=grad_accum_factor,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        warmup_ratio=warmup_ratio,
        lr_scheduler_type=scheduler,
        adam_beta2=adam_beta2,
        logging_steps=100,  
        load_best_model_at_end=True,
        metric_for_best_model="macro_f1",
        greater_is_better=True,
        fp16=torch.cuda.is_available(),
        report_to="none",
        save_total_limit=1,
        dataloader_num_workers=DATALOADER_NUM_WORKERS,
        remove_unused_columns=True,
    )

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_subset,
        eval_dataset=tokenized_val,
        data_collator=default_data_collator,
        compute_metrics=compute_metrics,
        callbacks=callbacks,
    )

    trainer.train()
    metrics = trainer.evaluate()
    return float(metrics.get("eval_macro_f1", 0.0))

# Main Block
if __name__ == "__main__":
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(CACHE_DIR, exist_ok=True)

    cleaned_datasets = load_and_preprocess_data(os.path.join(CACHE_DIR, "cleaned_dataset"))

    # Prepare validation subset 
    full_validation_df = cleaned_datasets["validation"].to_pandas()
    validation_subset_df = full_validation_df.sample(frac=0.1, random_state=VALIDATION_SUBSET_SEED)
    cleaned_datasets["validation"] = Dataset.from_pandas(validation_subset_df)

    if "__index_level_0__" in cleaned_datasets["validation"].column_names:
        cleaned_datasets["validation"] = cleaned_datasets["validation"].remove_columns(["__index_level_0__"])

    print(f"Using fixed validation subset with size: {len(validation_subset_df)}\n")

    # Start of Optuna Search Call
    # These are initial paramaters derived from the last optuna search
    initial_params = { 
        "learning_rate": 0.00013079373279230666,
        "weight_decay": 2.925368559272793e-06,
        "adam_beta2": 0.9988092277749205,
        "dropout": 0.08246747869081807,
        "warmup_ratio": 0.09684324847954481,
        "scheduler": "linear",
        "batch_size": 64,
        "grad_accum_factor": 1,
        "max_seq_len": 256,
    }

    # SQLite Database for the Study
    storage = optuna.storages.RDBStorage(url=f"sqlite:///{DB_PATH}", heartbeat_interval=60)
    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2, interval_steps=1)
    study = optuna.create_study(
        study_name="roberta-disinfo-hpo",
        storage=storage,
        direction="maximize",
        pruner=pruner,
        load_if_exists=True,  
    )

    study.enqueue_trial(initial_params, skip_if_exists=True)
    print(f"‚úÖ Enqueued your initial parameters as the first trial.")
    print(f"üíæ Study progress will be saved to and loaded from: {DB_PATH}")


    def print_best_callback(study, trial):
        print(f"\n‚úÖ Trial {trial.number} Finished | Value: {trial.value:.4f}")
        if study.best_trial:
            print(f"üèÜ Best Trial So Far: #{study.best_trial.number} | Best Value: {study.best_trial.value:.4f}")


    try:
        study.optimize(
            lambda trial: objective(trial, cleaned_datasets),
            n_trials=NUM_TRIALS,
            timeout=OPTUNA_TIMEOUT,
            gc_after_trial=True,
            callbacks=[print_best_callback],
        )
    except (KeyboardInterrupt, optuna.exceptions.TrialPruned):
        print("\nüõë Optimization interrupted or pruned.")

    if study.best_trial:
        print("\n\nüéâ ===== OPTIMIZATION FINISHED ===== üéâ")
        print(f" üèÜ Best Value (macro_f1): {study.best_trial.value:.4f}")
        print(" üìã Best Params:")
        for key, value in study.best_trial.params.items():
            print(f" - {key}: {value}")

        # Save study object (optional, since DB already has the data)
        study_path = os.path.join(OUTPUT_DIR, "optuna_study.pkl")
        with open(study_path, "wb") as f:
            pickle.dump(study, f)
        print(f"\nüíæ Full study object saved to: {study_path}")
    else:
        print("\n No trials were completed.")
